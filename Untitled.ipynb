{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b551bc8c",
   "metadata": {},
   "source": [
    "# RetroMAE\n",
    "Codebase for **[RetroMAE](https://arxiv.org/abs/2205.12035)** and beyond.\n",
    "\n",
    "# What's New\n",
    "- :fire: Oct. 2022, [RetroMAE: Pre-Training Retrieval-oriented Language Models Via\n",
    "Masked Auto-Encoder](https://arxiv.org/abs/2205.12035) is accepted to **EMNLP 2022**; SOTA performances on MS MARCO and BEIR from a BERT-base scale dense retriever!\n",
    "- :fire: Nov. 2022, [RetroMAE v2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models](https://arxiv.org/abs/2211.08769) is now on ArXiv. Another big stride forward from v1 and major improvements on MS MARCO and BEIR! Models and code are coming soon!\n",
    "\n",
    "\n",
    "## Released Models\n",
    "We have uploaded some checkpoints to Huggingface Hub. \n",
    "\n",
    "| Model | Description | Link  |\n",
    "|---|---|---|\n",
    "|RetroMAE | Pre-trianed on the wikipedia and bookcorpus | [Shitao/RetroMAE](https://huggingface.co/Shitao/RetroMAE) | \n",
    "|RetroMAE_MSMARCO | Pre-trianed on the MSMARCO passage | [Shitao/RetroMAE_MSMARCO](https://huggingface.co/Shitao/RetroMAE_MSMARCO) | \n",
    "|RetroMAE_MSMARCO_finetune |Finetune the RetroMAE_MSMARCO on the MSMARCO passage data | [Shitao/RetroMAE_MSMARCO_finetune](https://huggingface.co/Shitao/RetroMAE_MSMARCO_finetune) | \n",
    "|RetroMAE_MSMARCO_distill | Finetune the RetroMAE_MSMARCO on the MSMARCO passage data by minimizing the KL-divergence with the cross-encoder　| coming soon | \n",
    "|RetroMAE_BEIR | Finetune the RetroMAE on the MSMARCO passage data for BEIR (use the official negatives provided by BEIR)　| [Shitao/RetroMAE_BEIR](https://huggingface.co/Shitao/RetroMAE_BEIR) | \n",
    "\n",
    "You can load them easily using the identifier strings. For example:\n",
    "```python\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained('Shitao/RetroMAE')\n",
    "```\n",
    "\n",
    "## State of the Art Performance\n",
    "RetroMAE can provide a strong initialization of dense retriever; after fine-tuned with in-domain data, it\n",
    "gives rise to a high-quality supervised retrieval performance in the corresponding scenario. \n",
    "Besides, It substantially improves the pre-trained model's transferability, which helps to result in superior zero-shot performances on out-of-domain datasets.\n",
    "\n",
    "### MSMARCO Passage\n",
    "- Model pre-trained on wikipedia and bookcorpus:\n",
    "\n",
    "| Model | MRR@10 | Recall@1000 |\n",
    "|---|---|---|\n",
    "|Bert | 0.346 | 0.964 |\n",
    "|RetroMAE | **0.382** | **0.981** |\n",
    "\n",
    "- Model pre-trained on MSMARCO:\n",
    "\n",
    "| Model             | MRR@10 | Recall@1000 |\n",
    "|-------------------|---|---|\n",
    "| coCondenser         | 0.382 | 0.984 | \n",
    "| RetroMAE          | 0.393 | 0.985 | \n",
    "| RetroMAE(distillation) | **0.416** | **0.988** | \n",
    "\n",
    "### BEIR Benchemark\n",
    "\n",
    "| Model             | Avg NDCG@10 (18 datasets) |\n",
    "|-------------------|---|\n",
    "| Bert         | 0.371 | \n",
    "| Condenser       | 0.407 | \n",
    "| RetroMAE       | **0.452** | \n",
    "\n",
    "## Installation\n",
    "```\n",
    "git clone https://github.com/staoxiao/RetroMAE.git\n",
    "cd RetroMAE\n",
    "pip install .\n",
    "```\n",
    "For development, install as editable:\n",
    "\n",
    "```\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "## Workflow\n",
    "This repo includes two functions: pre-train and finetune. Firstly, train the RetroMAE on general dataset\n",
    " (or downstream dataset) with mask language modeling loss. Then finetune the RetroMAE on \n",
    " downstream dataset with contrastive loss. To achieve a better performance, you also can finetune the \n",
    " RetroMAE by distillation the scores provided by cross-encoder. **Detailed workflow please refer to our examples.** \n",
    "\n",
    "### Pretrain\n",
    "```\n",
    "python -m torch.distributed.launch --nproc_per_node 8 \\\n",
    "  -m pretrain.run \\\n",
    "  --output_dir {path to save ckpt} \\\n",
    "  --data_dir {your data} \\\n",
    "  --do_train True \\\n",
    "  --model_name_or_path bert-base-uncased \n",
    "```\n",
    "\n",
    "### Finetune\n",
    "```\n",
    "python -m torch.distributed.launch --nproc_per_node 8 \\\n",
    "-m bi_encoder.run \\\n",
    "--output_dir {path to save ckpt} \\\n",
    "--model_name_or_path Shitao/RetroMAE \\\n",
    "--do_train  \\\n",
    "--corpus_file ./data/BertTokenizer_data/corpus \\\n",
    "--train_query_file ./data/BertTokenizer_data/train_query \\\n",
    "--train_qrels ./data/BertTokenizer_data/train_qrels.txt \\\n",
    "--neg_file ./data/train_negs.tsv \n",
    "```\n",
    "\n",
    "\n",
    "## Examples\n",
    "\n",
    "[Pre-train](examples/pretrain/README.md)  \n",
    "[Finetune on MSMARCO Passage](examples/msmarco/README.md)  \n",
    "[BEIR Benchemark](examples/BEIR/README.md)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4657da40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python397jvsc74a57bd0ffde00659c2c70b24796679565a8d0c2bddda69f6163623b7f1ec0cb267d4804"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
